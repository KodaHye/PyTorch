{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce50d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile\n",
    "import glob\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91327814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 폴더가 없으면 작성한다\n",
    "data_dir = \"./data/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aab6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 폴더가 없으면 작성한다\n",
    "vocab_dir = \"./vocab/\"\n",
    "if not os.path.exists(vocab_dir):\n",
    "    os.mkdir(vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa46b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights 폴더가 없으면 작성한다\n",
    "weights_dir = \"./weights/\"\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e56c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./vocab/bert-base-uncased-vocab.txt',\n",
       " <http.client.HTTPMessage at 0x1d556791f28>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어집: vocabulary 다운로드\n",
    "\n",
    "# 'bert-base-uncased': \n",
    "# https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "\n",
    "save_path=\"./vocab/bert-base-uncased-vocab.txt\"\n",
    "url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\"\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2773aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 학습된 모델 'bert-base-uncased'\n",
    "# https://github.com/huggingface/pytorch-pretrained-BERT/\n",
    "# https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\n",
    "\n",
    "# 다운로드\n",
    "save_path = \"./weights/bert-base-uncased.tar.gz\"\n",
    "url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\"\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "# 압축 해제\n",
    "archive_file = \"./weights/bert-base-uncased.tar.gz\"  # Uncased는 소문자 모드라는 의미입니다\n",
    "tar = tarfile.open(archive_file, 'r:gz')\n",
    "tar.extractall('./weights/')  # 압축 해제\n",
    "tar.close()  # 파일 닫기\n",
    "\n",
    "# \"weights\" 폴더 내에 \"pytorch_model.bin\"와 \"bert_config.json\"가 생성됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1617c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정을 config.json에서 읽어들여, JSON 사전 변수를 오브젝트 변수로 변환\n",
    "import json\n",
    "\n",
    "config_file = \"./weights/bert_config.json\"\n",
    "\n",
    "# 파일을 열어 JSON으로 읽기\n",
    "json_file = open(config_file, 'r')\n",
    "config = json.load(json_file)\n",
    "\n",
    "# 출력 확인\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 적는 것은 번거롭다...\n",
    "config['hidden_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca39fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 변수를 오브젝트 변수로\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c348a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT용으로 LayerNormalization 층을 정의합니다.\n",
    "# 세부 구현을 TensorFlow에 맞추고 있습니다.\n",
    "class BertLayerNorm(nn.Module):\n",
    "    \"\"\"LayerNormalization층 \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))  # weight에 대한 것\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))  # bias에 대한 것\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 Embeddings 모듈입니다\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"문장의 단어 ID열과, 첫번째인지 두번째 문장인지의 정보를, 내장 벡터로 변환한다\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "        # 3개의 벡터 표현 내장\n",
    "\n",
    "        # Token Embedding: 단어 ID를 단어 벡터로 변환, \n",
    "        # vocab_size = 30522로, BERT의 학습된 모델에 사용된 vocabulary 양\n",
    "        # hidden_size = 768로, 특징량 벡터의 길이는 768\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        # (주석)padding_idx=0은, idx=0의 단어 벡터는 0으로 한다. BERT의 vocabulary의 idx=0은 [PAD]임.\n",
    "\n",
    "        # Transformer Positional Embedding: 위치 정보 텐서를 벡터로 변환\n",
    "        # Transformer의 경우는 sin, cos로 이루어진 고정값이지만, BERT 학습시킴\n",
    "        # max_position_embeddings = 512로, 문장의 길이는 512단어\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        # Sentence Embedding: 첫번째, 두번째 문장을 벡터로 변환\n",
    "        # type_vocab_size = 2\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # 작성한 LayerNormalization층\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # Dropout　'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids:  [batch_size, seq_len] 문장의 단어 ID를 나열\n",
    "        token_type_ids: [batch_size, seq_len] 각 단어가 1번째 문장인지, 2번째 문장인지를 나타내는 id\n",
    "        '''\n",
    "\n",
    "        # 1. Token Embeddings\n",
    "        # 단어 ID를 단어 벡터로 변환\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        # 2. Sentence Embedding\n",
    "        # token_type_ids가 없는 경우는 문장의 모든 단어를 첫번째 문장으로 하여, 0으로 설정\n",
    "        # input_ids와 같은 크기의 제로 텐서를 작성\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 3. Transformer Positional Embedding: \n",
    "        # [0, 1, 2 ・・・]로 문장의 길이만큼 숫자가 하나씩 올라간다\n",
    "        # [batch_size, seq_len]의 텐서 position_ids를 작성\n",
    "        # position_ids를 입력해서, position_embeddings 층에서 768차원의 텐서를 꺼낸다\n",
    "        seq_length = input_ids.size(1)  # 문장의 길이\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        # 3개의 내장 텐서를 더한다 [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # LayerNormalization과 Dropout을 실행\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERT의 BertLayer 모듈입니다. Transformer가 됩니다'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "\n",
    "        # Self-Attention 부분\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        # Self-Attention의 출력을 처리하는 전결합층\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        # Self-Attention에 의한 특징량과 BertLayer에 원래의 입력을 더하는 층\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states: Embedder 모듈의 출력 텐서 [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_flg: Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_show일 경우, attention_probs도 반환한다'''\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "            return layer_output  # [batch_size, seq_length, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9252f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    '''BertLayer 모듈의 Self-Attention 부분입니다'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        input_tensor: Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹입니다\n",
    "        attention_show_flg: Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_show일 경우, attention_probs도 반환한다'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f70737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttention의 Self-Attention입니다'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads': 12\n",
    "\n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads)  # 768/12=64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size  # = 'hidden_size': 768\n",
    "\n",
    "        # Self-Attention의 특징량을 작성하는 전결합층\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        '''multi-head Attention용으로 텐서의 형태를 변환한다\n",
    "        [batch_size, seq_len, hidden] → [batch_size, 12, seq_len, hidden/12] \n",
    "        '''\n",
    "        new_x_shape = x.size()[\n",
    "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states: Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹입니다\n",
    "        attention_show_flg: Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        # 입력을 전결합층에서 특징량 변환(주의, multi-head Attention 전부를 한꺼번에 변환하고 있습니다)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # multi-head Attention용으로 텐서 형태를 변환\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # 특징량끼리를 곱해서 비슷한 정도를 Attention_scores로 구한다\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # 마스크가 있는 부분에 마스크를 적용합니다\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        # (비고)\n",
    "        # 마스크는 곱셈이 아니라 덧셈이 직관적이지만, 그 후에 Softmax로 정규화하므로,\n",
    "        # 마스크된 부분은 -inf로 합니다. attention_mask에는, 0이나-inf가\n",
    "        # 원래 들어 있으므로 덧셈으로 합니다.\n",
    "\n",
    "        # Attention을 정규화한다\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # 드롭아웃합니다\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Attention Map을 곱합니다\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # multi-head Attention의 텐서 형을 원래대로 되돌림\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[\n",
    "            :-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        # attention_show일 경우, attention_probs도 반환한다\n",
    "        if attention_show_flg == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae660dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttention의 출력을 처리하는 전결합층입니다'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states: BertSelfAttention의 출력 텐서\n",
    "        input_tensor: Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unit라는 활성화 함수입니다.\n",
    "    LeLU가 0으로 거칠고 불연속적이므로, 이를 연속적으로 매끄럽게 한 형태의 LeLU입니다.\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈의 FeedForward입니다'''\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 전결합층: 'hidden_size': 768, 'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        \n",
    "        # 활성화 함수 gelu\n",
    "        self.intermediate_act_fn = gelu\n",
    "            \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states:  BertAttention의 출력 텐서\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)  # GELU에 의한 활성화\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈의 FeedForward입니다'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "\n",
    "        # 전결합층: 'intermediate_size': 3072, 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states:  BertIntermediate의 출력 텐서\n",
    "        input_tensor: BertAttention의 출력 텐서\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer 모듈의 반복 부분입니다\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayer 모듈의 반복 부분입니다'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        # config.num_hidden_layers의 값. 즉, 12개의 BertLayer 모듈을 만듭니다\n",
    "        self.layer = nn.ModuleList([BertLayer(config)\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states: Embeddings 모듈의 출력\n",
    "        attention_mask: Transformer의 마스크와 동일한 기능의 마스킹입니다\n",
    "        output_all_encoded_layers: 반환 값을 전체 TransformerBlock 모듈의 출력으로 할지, \n",
    "        최후 층만으로 한정할지의 플래그.\n",
    "        attention_show_flg: Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "\n",
    "        # 반환 값으로 사용할 리스트\n",
    "        all_encoder_layers = []\n",
    "\n",
    "        # BertLayer 모듈의 처리를 반복\n",
    "        for layer_module in self.layer:\n",
    "\n",
    "            if attention_show_flg == True:\n",
    "                '''attention_show의 경우, attention_probs도 반환한다'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "            elif attention_show_flg == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "\n",
    "            # 반환값으로 BertLayer에서 출력된 특징량을 12층 분, 모두 사용할 경우의 처리\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "\n",
    "        # 반환값으로 최후의 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "\n",
    "        # attention_show의 경우, attention_probs(마지막 12단)도 반환한다\n",
    "        if attention_show_flg == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return all_encoder_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173740e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''입력 문장의 첫번째 단어[cls]의 특징량을 반환하고 유지하기 위한 모듈'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        # 전결합층, 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1번째 단어의 특징량을 취득\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        # 전결합층에서 특징량 변환\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        # 활성화 함수 Tanh를 계산\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb905d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "\n",
    "# 입력 단어 ID열, batch_size는 두가지\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"입력 단어 ID열의 텐서 크기: \", input_ids.shape)\n",
    "\n",
    "# 마스크\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print(\"입력 마스크의 텐서 크기: \", attention_mask.shape)\n",
    "\n",
    "# 문장의 ID. 두 미니 배치 각각에 대해, 0은 첫번째 문장을, 1은 2번째 문장을 나타냄\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"입력 문장 ID의 텐서 크기: \", token_type_ids.shape)\n",
    "\n",
    "\n",
    "# BERT의 각 모듈을 준비\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# 마스크 변형　[batch_size, 1, 1, seq_length]으로 한다\n",
    "# Attention을 적용하지 않는 부분은 마이너스 무한으로 하고 싶으므로, -10000을 곱하고 있습니다\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print(\"확장된 마스크의 텐서 크기: \", extended_attention_mask.shape)\n",
    "\n",
    "# 순전파한다\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print(\"BertEmbeddings의 출력 텐서 크기: \", out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2는, [minibatch, seq_length, embedding_dim]가 12개의 리스트\n",
    "print(\"BertEncoder 최후 층의 출력 텐서 크기: \", out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1])  # out2는 12층의 특징량의 리스트가 되어 있으므로 가장 마지막을 사용\n",
    "print(\"BertPooler의 출력 텐서 크기: \", out3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6224a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''모듈을 전부 연결한 BERT 모델'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        # 3가지 모듈을 작성\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids:  [batch_size, sequence_length] 문장의 단어 ID를 나열\n",
    "        token_type_ids:  [batch_size, sequence_length] 각 단어가 1번째 문장인지, 2번째 문장인지를 나타내는 id\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        output_all_encoded_layers: 최후 출력에 12단의 Transformer의 전부를 리스트로 반환할지, 최후만인지를 지정\n",
    "        attention_show_flg: Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "\n",
    "        # Attention의 마스크와 첫번째, 두번째 문장의 id가 없으면 작성한다\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # 마스크 변형 [minibatch, 1, 1, seq_length]으로 한다\n",
    "        # 나중에 multi-head Attention에서 사용할 수 있는 형태로 하고 싶으므로\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # 마스크는 0, 1이지만 소프트 맥스를 계산할 때 마스크가 되도록, 0과 -inf으로 한다\n",
    "        # -inf 대신 -10000으로 해 둡니다\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # 순전파시킨다\n",
    "        # BertEmbeddins 모듈\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "\n",
    "        # BertLayer 모듈(Transformer)을 반복하는 BertEncoder 모듈\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_show의 경우, attention_probs도 반환한다'''\n",
    "\n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # BertPooler 모듈\n",
    "        # encoder의 맨 마지막 BertLayer에서 출력된 특징량을 사용\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "\n",
    "        # output_all_encoded_layers가 False인 경우는 리스트가 아니라, 텐서를 반환\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        # attention_show의 경우, attention_probs(가장 마지막)도 반환한다\n",
    "        if attention_show_flg == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return encoded_layers, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "# 입력을 준비\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERT 모델을 만들기\n",
    "net = BertModel(config)\n",
    "\n",
    "# 순전파시킨다\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "print(\"encoded_layers의 텐서 크기: \", encoded_layers.shape)\n",
    "print(\"pooled_output의 텐서 크기: \", pooled_output.shape)\n",
    "print(\"attention_probs의 텐서 크기: \", attention_probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f154d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 로드\n",
    "weights_path = \"./weights/pytorch_model.bin\"\n",
    "loaded_state_dict = torch.load(weights_path)\n",
    "\n",
    "for s in loaded_state_dict.keys():\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c31317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 준비\n",
    "net = BertModel(config)\n",
    "net.eval()\n",
    "\n",
    "# 현재 네트워크 모델의 파라미터 이름\n",
    "param_names = []  # 파라미터 이름을 저장해 나간다\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    param_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cf71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict의 이름이 다르므로 앞쪽부터 순서대로 대입한다\n",
    "# 이번에는 파라미터 이름이 달라도, 대응하는 것은 동일한 순서로 되어 있습니다\n",
    "\n",
    "# 현재 네트워크 정보를 복사하여 새로운 state_dict을 작성\n",
    "new_state_dict = net.state_dict().copy()\n",
    "\n",
    "# 새로운 state_dict에 학습된 값을 대입\n",
    "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
    "    name = param_names[index]  # 현재 네트워크의 파라미터명을 취득\n",
    "    new_state_dict[name] = value  # 값을 넣는다\n",
    "    print(str(key_name)+\"→\"+str(name))  # 어디로 들어갔는지 표시\n",
    "\n",
    "    # 현재 네트워크의 파라미터를 전부 로드하면 끝낸다\n",
    "    if index+1 >= len(param_names):\n",
    "        break\n",
    "\n",
    "# 새로운 state_dict를 구축한 BERT 모델에 제공\n",
    "net.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230130ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 파일을 읽기\n",
    "import collections\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"text 형식의 vocab 파일의 내용을 사전에 저장합니다\"\"\"\n",
    "    vocab = collections.OrderedDict()  # (단어, id) 순서의 사전 변수\n",
    "    ids_to_tokens = collections.OrderedDict()  # (id, 단어) 순서의 사전 변수\n",
    "    index = 0\n",
    "\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "\n",
    "            # 저장\n",
    "            vocab[token] = index\n",
    "            ids_to_tokens[index] = token\n",
    "            index += 1\n",
    "\n",
    "    return vocab, ids_to_tokens\n",
    "\n",
    "\n",
    "# 실행\n",
    "vocab_file = \"./vocab/bert-base-uncased-vocab.txt\"\n",
    "vocab, ids_to_tokens = load_vocab(vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a51508",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import BasicTokenizer, WordpieceTokenizer\n",
    "\n",
    "# BasicTokenizer, WordpieceTokenizer는, 참고 문헌[2] 그대로입니다\n",
    "# https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py\n",
    "# sub-word로 단어 분할을 실시하는 클래스들입니다.\n",
    "class BertTokenizer(object):\n",
    "    '''BERT용의 문장 단어 분할 클래스를 구현'''\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        '''\n",
    "        vocab_file: vocabulary에의 경로\n",
    "        do_lower_case: 전처리에서 단어를 소문자로 바꾸는지 여부\n",
    "        '''\n",
    "\n",
    "        # vocabulary의 로드\n",
    "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
    "\n",
    "        # 분할 처리 함수를 \"utils\" 폴더에서 imoprt, sub-word로 단어 분할을 실시\n",
    "        never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    "        # (주석)위 단어는 도중에 분할하지 않는다. 이를 통해 하나의 단어로 간주함\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                              never_split=never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        '''문장의 단어를 분할하는 함수'''\n",
    "        split_tokens = []  # 분할 후 단어들\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"분할된 단어 목록을 ID로 변환하는 함수\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"ID를 단어로 변환하는 함수\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장1: 은행 계좌에 접근했습니다.\n",
    "text_1 = \"[CLS] I accessed the bank account. [SEP]\"\n",
    "\n",
    "# 문장2: 그는 보증금을 은행 계좌로 이체했습니다.\n",
    "text_2 = \"[CLS] He transferred the deposit money into the bank account. [SEP]\"\n",
    "\n",
    "# 문장3: 우리는 강변에서 축구를 합니다.\n",
    "text_3 = \"[CLS] We play soccer at the bank of the river. [SEP]\"\n",
    "\n",
    "# 단어 분할 Tokenizer를 준비\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# 문장의 단어를 분할\n",
    "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
    "tokenized_text_3 = tokenizer.tokenize(text_3)\n",
    "\n",
    "# 확인\n",
    "print(tokenized_text_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 ID로 변환하기\n",
    "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "indexed_tokens_3 = tokenizer.convert_tokens_to_ids(tokenized_text_3)\n",
    "\n",
    "# 각 문장의 bank 위치\n",
    "bank_posi_1 = np.where(np.array(tokenized_text_1) == \"bank\")[0][0]  # 4\n",
    "bank_posi_2 = np.where(np.array(tokenized_text_2) == \"bank\")[0][0]  # 8\n",
    "bank_posi_3 = np.where(np.array(tokenized_text_3) == \"bank\")[0][0]  # 6\n",
    "\n",
    "# seqId(첫번째인지 2번째 문장인지는 이번에는 필요 없음)\n",
    "\n",
    "# 리스트를 PyTorch의 텐서로\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])\n",
    "tokens_tensor_3 = torch.tensor([indexed_tokens_3])\n",
    "\n",
    "# bank의 단어 id\n",
    "bank_word_id = tokenizer.convert_tokens_to_ids([\"bank\"])[0]\n",
    "\n",
    "# 확인\n",
    "print(tokens_tensor_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addb8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT로 처리\n",
    "with torch.no_grad():\n",
    "    encoded_layers_1, _ = net(tokens_tensor_1, output_all_encoded_layers=True)\n",
    "    encoded_layers_2, _ = net(tokens_tensor_2, output_all_encoded_layers=True)\n",
    "    encoded_layers_3, _ = net(tokens_tensor_3, output_all_encoded_layers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank의 초기 단어 벡터 표현\n",
    "# 이것은 Embeddings 모듈에서 꺼내, 단어 bank의 id에 따른 단어 벡터이므로 세 문장에서 공통\n",
    "bank_vector_0 = net.embeddings.word_embeddings.weight[bank_word_id]\n",
    "\n",
    "# 문장1의 BertLayer 모듈 1단에서 출력되는 bank의 특징량 벡터\n",
    "bank_vector_1_1 = encoded_layers_1[0][0, bank_posi_1]\n",
    "\n",
    "# 문장1의 BertLayer 모듈 최후 12단에서 출력되는 bank의 특징량 벡터\n",
    "bank_vector_1_12 = encoded_layers_1[11][0, bank_posi_1]\n",
    "\n",
    "# 문장2, 3도 마찬가지로\n",
    "bank_vector_2_1 = encoded_layers_2[0][0, bank_posi_2]\n",
    "bank_vector_2_12 = encoded_layers_2[11][0, bank_posi_2]\n",
    "bank_vector_3_1 = encoded_layers_3[0][0, bank_posi_3]\n",
    "bank_vector_3_12 = encoded_layers_3[11][0, bank_posi_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 계산\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"bank의 초기 벡터와 문장1의 1단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_0, bank_vector_1_1, dim=0))\n",
    "print(\"bank의 초기 벡터와 문장1의 12단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_0, bank_vector_1_12, dim=0))\n",
    "\n",
    "print(\"문장1의 1층 bank와 문장2의 1단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_1_1, bank_vector_2_1, dim=0))\n",
    "print(\"문장1의 1층 bank와 문장3의 1단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_1_1, bank_vector_3_1, dim=0))\n",
    "\n",
    "print(\"문장1의 12층 bank와 문장2의 12단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_1_12, bank_vector_2_12, dim=0))\n",
    "print(\"문장1의 12층 bank와 문장3의 12단 bank의 유사도: \",\n",
    "      F.cosine_similarity(bank_vector_1_12, bank_vector_3_12, dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    '''BERT의 사전 학습 과제를 수행하는 어댑터 모듈'''\n",
    "\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "\n",
    "        # 사전 학습 과제: Masked Language Model용 모듈\n",
    "        self.predictions = MaskedWordPredictions(config)\n",
    "\n",
    "        # 사전 학습 과제: Next Sentence Prediction용 모듈\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        '''입력 정보\n",
    "        sequence_output:[batch_size, seq_len, hidden_size]\n",
    "        pooled_output:[batch_size, hidden_size]\n",
    "        '''\n",
    "        # 입력의 마스크된 각 단어가 어떤 단어인지를 판정\n",
    "        # 출력 [minibatch, seq_len, vocab_size]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        # 선두 단어의 특징량에서 1번째, 2번째 문장이 연결되어 있는지 판정\n",
    "        seq_relationship_score = self.seq_relationship(\n",
    "            pooled_output)  # 출력 [minibatch, 2]\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BERT의 사전 학습 과제를 수행하는 어댑터 모듈'''\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "\n",
    "        # 사전 학습 과제: Masked Language Model용 모듈\n",
    "        self.predictions = MaskedWordPredictions(config)\n",
    "\n",
    "        # 사전 학습 과제: Next Sentence Prediction용 모듈\n",
    "        self.seq_relationship = SeqRelationship(config, out_features=2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        '''입력 정보\n",
    "        sequence_output:[batch_size, seq_len, hidden_size]\n",
    "        pooled_output:[batch_size, hidden_size]\n",
    "        '''\n",
    "        # 입력의 마스크된 각 단어가 어떤 단어인지를 판정\n",
    "        # 출력 [batch_size, seq_len, hidden_size]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        # 선두 단어의 특징량에서 1번째, 2번째 문장이 연결되어 있는지 판정\n",
    "        seq_relationship_score = self.seq_relationship(\n",
    "            pooled_output)  # 출력 [batch_size, 2]\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24baa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 과제: Masked Language Model용 모듈\n",
    "class MaskedWordPredictions(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''사전 학습 과제: Masked Language Model용 모듈\n",
    "        원래의 [2] 구현에서는, BertLMPredictionHead이라는 이름입니다.\n",
    "        '''\n",
    "        super(MaskedWordPredictions, self).__init__()\n",
    "\n",
    "        # BERT에서 출력된 특징량을 변환하는 모듈(입출력 크기는 동일)\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # self.transform의 출력에서, 각 위치의 단어가 어떤 것인지를 알아맞히는 전결합층\n",
    "        self.decoder = nn.Linear(in_features=config.hidden_size,  # 'hidden_size': 768\n",
    "                                 out_features=config.vocab_size,  # 'vocab_size': 30522\n",
    "                                 bias=False)\n",
    "        # 바이어스 항\n",
    "        self.bias = nn.Parameter(torch.zeros(\n",
    "            config.vocab_size))  # 'vocab_size': 30522\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states: BERT에서의 출력[batch_size, seq_len, hidden_size]\n",
    "        '''\n",
    "        # BERT에서 출력된 특징량을 변환\n",
    "        # 출력 크기: [batch_size, seq_len, hidden_size]\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "\n",
    "        # 각 위치의 단어가 vocabulary의 어느 단어인지를 클래스 분류로 예측\n",
    "        # 출력 크기: [batch_size, seq_len, vocab_size]\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    '''MaskedWordPredictions에서, BERT의 특징량을 변환하는 모듈(입출력 크기는 동일)'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertPredictionHeadTransform, self).__init__()\n",
    "\n",
    "        # 전결합층 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        # 활성화 함수 gelu\n",
    "        self.transform_act_fn = gelu\n",
    "\n",
    "        # LayerNormalization\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        '''hidden_statesはsequence_output:[minibatch, seq_len, hidden_size]'''\n",
    "        # 전결합층에서 특징량 변환하여, 활성화 함수 gelu를 게산한 뒤, LayerNormalization을 수행\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 과제: Next Sentence Prediction용 모듈\n",
    "class SeqRelationship(nn.Module):\n",
    "    def __init__(self, config, out_features):\n",
    "        '''사전 학습 과제: Next Sentence Prediction용 모듈\n",
    "        원래 인수[2] 구현에서는, 특별히 크래스로 제공하고 있지 않음.\n",
    "        일반적인 전결합층에, 일부러 이름을 붙임.\n",
    "        '''\n",
    "        super(SeqRelationship, self).__init__()\n",
    "\n",
    "        # 선두 단어의 특징량에서 1번째, 2번째 문장이 연결되어 있는지 판정하는 클래스 분류의 전결합층\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, out_features)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        return self.seq_relationship(pooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(nn.Module):\n",
    "    '''BERT 모델에 사전 학습 과제용의 어댑터 모듈\n",
    "    BertPreTrainingHeads를 연결한 모델'''\n",
    "\n",
    "    def __init__(self, config, net_bert):\n",
    "        super(BertForMaskedLM, self).__init__()\n",
    "\n",
    "        # BERT 모듈\n",
    "        self.bert = net_bert\n",
    "\n",
    "        # 사전 학습 과제용 어댑터 모듈\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        '''\n",
    "        input_ids:  [batch_size, sequence_length] 문장의 단어 ID를 나열\n",
    "        token_type_ids:  [batch_size, sequence_length] 각 단어가 1번째 문장인지, 2번째 문장인지를 나타내는 id\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        '''\n",
    "\n",
    "        # BERT의 기본 모델 부분의 순전파\n",
    "        encoded_layers, pooled_output = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "        # 사전 학습 과제의 추론을 실시\n",
    "        prediction_scores, seq_relationship_score = self.cls(\n",
    "            encoded_layers, pooled_output)\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 기본 모델\n",
    "net_bert = BertModel(config)\n",
    "net_bert.eval()\n",
    "\n",
    "# 사전 학습 과제의 어댑터 모듈을 탑재한 BERT\n",
    "net = BertForMaskedLM(config, net_bert)\n",
    "net.eval()\n",
    "\n",
    "# 학습된 가중치를 로드\n",
    "weights_path = \"./weights/pytorch_model.bin\"\n",
    "loaded_state_dict = torch.load(weights_path)\n",
    "\n",
    "\n",
    "# 현재 네트워크 모델의 파라미터명\n",
    "param_names = []  # 파라미터 이름을 저장해 나간다\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param_names.append(name)\n",
    "\n",
    "\n",
    "# 현재 네트워크 정보를 복사하여 새로운 state_dict를 작성\n",
    "new_state_dict = net.state_dict().copy()\n",
    "\n",
    "# 새로운 state_dict에 학습된 값을 대입\n",
    "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
    "    name = param_names[index]  # 현재 네트워크의 파라미터명을 취득\n",
    "    new_state_dict[name] = value  # 값을 넣는다\n",
    "    print(str(key_name)+\"→\"+str(name))  # 어디로 들어갔는지 표시\n",
    "\n",
    "    # 현재 네트워크의 파라미터를 전부 로드하면 끝낸다\n",
    "    if index+1 >= len(param_names):\n",
    "        break\n",
    "\n",
    "# 새로운 state_dict를 구축한 BERT 모델에 제공\n",
    "net.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력할 문장을 준비\n",
    "text = \"[CLS] I accessed the bank account. [SEP] We play soccer at the bank of the river. [SEP]\"\n",
    "\n",
    "# 단어 분할 Tokenizer을 준비\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# 문장의 단어를 분할\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 마스크한다. 이번에는 13번째 단어의 bank를 마스크\n",
    "masked_index = 13\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "print(tokenized_text)  # 13번째 단어가 [MASK] 되어 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480269f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 ID로 변환한다\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 문장에 0을, 2번째 문장에 1을 넣어 문장 ID를 준비\n",
    "def seq2id(indexed_tokens):\n",
    "    '''띄어쓰기된 단어 ID열을 문장 ID로. [SEP]으로 나누기'''\n",
    "\n",
    "    segments_ids = []\n",
    "    seq_id = 0\n",
    "\n",
    "    for word_id in indexed_tokens:\n",
    "        segments_ids.append(seq_id)  # seq_id=o or 1을 추가\n",
    "\n",
    "        # [SEP]를 발견하면 2번째 문장이 되므로 이후 id를 1로\n",
    "        if word_id == 102:  # ID 102가 [SEP]이다\n",
    "            seq_id = 1\n",
    "\n",
    "    return segments_ids\n",
    "\n",
    "\n",
    "# 실행\n",
    "segments_ids = seq2id(indexed_tokens)\n",
    "print(segments_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6305a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 추론\n",
    "\n",
    "# 리스트를 PyTorch의 텐서로 하여 모델에 입력\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    prediction_scores, seq_relationship_score = net(\n",
    "        tokens_tensor, segments_tensors)\n",
    "\n",
    "# 추론한 ID를 단어로 되돌리기\n",
    "predicted_index = torch.argmax(prediction_scores[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_relationship_score)\n",
    "print(torch.sigmoid(seq_relationship_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fa1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 출력을 보면, 클래스0: 두 문장이 의미를 가지는 연속한 것, 클래스1: 두 문장은 무관계\n",
    "# 중에서, 클래스1의 무관계로 판정하고 있음.\n",
    "\n",
    "# 입력 문장은\n",
    "# text = \"[CLS] I accessed the bank account. [SEP] We play soccer at the bank of the river. [SEP]\"\n",
    "# 와 무관계였으므로, 제대로 Next Sentence Prediction가 동작했음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88442e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
